{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "4f42d3dee551ac652da33d55bc5f4f97490afc49480a1414a7292e879edb9b67"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "tutorial at:\n",
    "\n",
    "https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/tour-of-beam/reading-and-writing-data.ipynb#scrollTo=sQUUi4H9s-g2\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "make files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/my-text-file-1.txt\n",
    "This is just a plain text file, UTF-8 strings are allowed ğŸ‰.\n",
    "Each line in the file is one element in the PCollection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/my-text-file-2.txt\n",
    "There are no guarantees on the order of the elements.\n",
    "à¸…^â€¢ï»Œâ€¢^à¸…"
   ]
  },
  {
   "source": [
    "Read files, each line is an element in th p collection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "input_files = 'data/*.txt'\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | 'Read files' >> beam.io.ReadFromText(input_files)\n",
    "        | 'Print contents' >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "source": [
    "write files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name_prefix = 'outputs/file'\n",
    "\n",
    "lines_to_write = [\n",
    "    'Each element must be a string.',\n",
    "    'It writes one element per line.',\n",
    "    'There are no guarantees on the line order.',\n",
    "    'The data might be written into multiple files.',\n",
    "]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    (\n",
    "        pipeline\n",
    "        | 'Create file lines' >> beam.Create(lines_to_write)\n",
    "        | 'Write to files' >> beam.io.WriteToText(\n",
    "            output_file_name_prefix,\n",
    "            file_name_suffix='.txt')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the output files and contents.\n",
    "!head outputs/file*.txt"
   ]
  },
  {
   "source": [
    "Reading from an iterable\n",
    "\n",
    "The easiest way to create elements is using FlatMap.\n",
    "A common way is having a generator function. This could take an input and expand it into a large amount of elements. The nice thing about generators is that they don't have to fit everything into memory like a list, they simply yield elements as they process them.\n",
    "For example, let's define a generator called count, that yields the numbers from 0 to n. We use Create for the initial n value(s) and then exapand them with FlatMap."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "def count(n):\n",
    "  for i in range(n):\n",
    "    yield i\n",
    "\n",
    "n = 5\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Create inputs' >> beam.Create([n])\n",
    "      | 'Generate elements' >> beam.FlatMap(count)\n",
    "      | 'Print elements' >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "source": [
    "Creating an input transform\n",
    "\n",
    "For a nicer interface, we could abstract the Create and the FlatMap into a custom PTransform. This would give a more intuitive way to use it, while hiding the inner workings.\n",
    "We create a new class that inherits from beam.PTransform. Any input from the generator function, like n, becomes a class field. The generator function itself would now become a staticmethod. And we can hide the Create and FlatMap in the expand method.\n",
    "Now we can use our transform in a more intuitive way, just like ReadFromText."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class Count(beam.PTransform):\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  @staticmethod\n",
    "  def count(n):\n",
    "    for i in range(n):\n",
    "      yield i\n",
    "\n",
    "  def expand(self, pcollection):\n",
    "    return (\n",
    "        pcollection\n",
    "        | 'Create inputs' >> beam.Create([self.n])\n",
    "        | 'Generate elements' >> beam.FlatMap(Count.count)\n",
    "    )\n",
    "\n",
    "n = 3\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | f'Count to {n}' >> Count(n)\n",
    "      | 'Print elements' >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "source": [
    "Example: Reading CSV files\n",
    "\n",
    "Lets say we want to read CSV files to get elements as Python dictionaries. We like how ReadFromText expands a file pattern, but we might want to allow for multiple patterns as well.\n",
    "We create a ReadCsvFiles transform, which takes a list of file_patterns as input. It expands all the glob patterns, and then, for each file name it reads each row as a dict using the csv.DictReader module."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/penguins.csv\n",
    "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n",
    "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n",
    "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n",
    "1,0.5236363636363636,0.5714285714285713,0.3389830508474576,0.2222222222222222\n",
    "1,0.6509090909090909,0.7619047619047619,0.4067796610169492,0.3333333333333333\n",
    "2,0.509090909090909,0.011904761904761862,0.6610169491525424,0.5\n",
    "2,0.6509090909090909,0.38095238095238104,0.9830508474576272,0.8333333333333334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "class ReadCsvFiles(beam.PTransform):\n",
    "  def __init__(self, file_patterns):\n",
    "    self.file_patterns = file_patterns\n",
    "\n",
    "  @staticmethod\n",
    "  def read_csv_lines(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "      for row in csv.DictReader(f):\n",
    "        yield dict(row)\n",
    "\n",
    "  def expand(self, pcollection):\n",
    "    return (\n",
    "        pcollection\n",
    "        | 'Create file patterns' >> beam.Create(self.file_patterns)\n",
    "        | 'Expand file patterns' >> beam.FlatMap(glob.glob)\n",
    "        | 'Read CSV lines' >> beam.FlatMap(self.read_csv_lines)\n",
    "    )\n",
    "\n",
    "input_patterns = ['data/*.csv']\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Read CSV files' >> ReadCsvFiles(input_patterns)\n",
    "      | 'Print elements' >> beam.Map(print)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "def read_csv_lines(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in csv.DictReader(f):\n",
    "            yield dict(row)\n",
    "\n",
    "input_patterns = ['data/*.csv']\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Create file patterns' >> beam.Create(input_patterns)\n",
    "      | 'Expand file patterns' >> beam.FlatMap(glob.glob)\n",
    "      | 'Read CSV lines' >> beam.FlatMap(read_csv_lines)\n",
    "      | 'Print elements' >> beam.Map(print)\n",
    "  )"
   ]
  }
 ]
}