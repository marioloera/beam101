{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Writing data\n",
    "\n",
    "Your might want to write your data in various output formats. Take a look at the\n",
    "[Built-in I/O Transforms](https://beam.apache.org/documentation/io/built-in)\n",
    "page for a list of all the available I/O transforms in Beam.\n",
    "\n",
    "If none of those work for you, you might need to create your own output transform.\n",
    "\n",
    "> ℹ️ For a more in-depth guide, take a look at the\n",
    "[Developing a new I/O connector](https://beam.apache.org/documentation/io/developing-io-overview) page.\n",
    "\n",
    "\n",
    "## Creating an output transform\n",
    "\n",
    "The most straightforward way to write data would be to use a `Map` transform to write each element into our desired output format. In most cases, however, this would result in a lot of overhead creating, connecting to, and/or deleting resources.\n",
    "\n",
    "Instead, most data services are optimized to write _batches_ of elements at a time. Batch writes only connects to the service once, and can load many elements at a time.\n",
    "\n",
    "Here, we discuss two common ways of batching elements for optimized writes: _fixed-sized batches_, and\n",
    "_[windows](https://beam.apache.org/documentation/programming-guide/#windowing)\n",
    "of elements_."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Writing fixed-sized batches\n",
    "\n",
    "If the order of the elements _is not_ important, we can simply create fixed-sized batches and write those independently.\n",
    "\n",
    "We can use\n",
    "[`GroupIntoBatches`](https://beam.apache.org/documentation/transforms/python/aggregation/groupintobatches)\n",
    "to get fixed-sized batches. Note that it expects `(key, value)` pairs. Since `GroupIntoBatches` is an _aggregation_, all the elements in a batch _must_ fit into memory for each worker.\n",
    "\n",
    "> ℹ️ `GroupIntoBatches` requires a `(key, value)` pair. For simplicity, this example uses a placeholder `None` key and discards it later. Depending on your data, there might be a key that makes more sense. Using a _balanced_ key, where each key contains around the same number of elements, may help parallelize the batching process.\n",
    "\n",
    "Let's create something similar to `WriteToText` but keep it simple with a unique identifier in the file name instead of the file count."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import glob\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "class WriteBatchesToFiles(beam.PTransform):\n",
    "  def __init__(self, file_name_prefix, file_name_suffix, batch_size):\n",
    "    self.file_name_prefix = file_name_prefix\n",
    "    self.file_name_suffix = file_name_suffix\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "  @staticmethod\n",
    "  def write_file(lines, file_name_prefix, file_name_suffix):\n",
    "    file_name = f\"{file_name_prefix}-{uuid.uuid4().hex}{file_name_suffix}\"\n",
    "    with open(file_name, 'w') as f:\n",
    "      for line in lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "  def expand(self, pcollection):\n",
    "\n",
    "    return (\n",
    "        pcollection\n",
    "        # For simplicity we key with `None` and discard it.\n",
    "        | 'Key with None' >> beam.WithKeys(lambda _: None)\n",
    "        | 'Group into batches' >> beam.GroupIntoBatches(self.batch_size)\n",
    "        | 'Discard key' >> beam.Values()\n",
    "        | 'Write file' >> beam.Map(\n",
    "            self.write_file,\n",
    "            file_name_prefix=self.file_name_prefix,\n",
    "            file_name_suffix=self.file_name_suffix,\n",
    "        )\n",
    "    )\n",
    "\n",
    "output_file_name_prefix = 'outputs/batch'\n",
    "file_name_suffix = '.txt'\n",
    "# Remove existing files matching the output file_name pattern.\n",
    "for f in glob.glob(f\"{output_file_name_prefix}*{file_name_suffix}\"):\n",
    "    os.remove(f)\n",
    "with beam.Pipeline() as pipeline:\n",
    "  (\n",
    "      pipeline\n",
    "      | 'Create file lines' >> beam.Create([\n",
    "          '1 Each element must be a string.',\n",
    "          '2 It writes one element per line.',\n",
    "          '3 There are no guarantees on the line order.',\n",
    "          '4 The data might be written into multiple files.',\n",
    "          '5 The data might be written into multiple files.',\n",
    "      ])\n",
    "      | 'Write batches to files' >> WriteBatchesToFiles(\n",
    "          output_file_name_prefix,\n",
    "          file_name_suffix=file_name_suffix,\n",
    "          batch_size=2,\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at the output files and contents.\n",
    "!head outputs/batch*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}